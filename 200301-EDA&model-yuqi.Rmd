---
title: "200301-EDA_and_model-yuqi"
author: "Yuqi Miao ym2771"
date: "3/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(survival)
require(quantreg)
require(glmnet)
require(MASS)
require(pROC)

set.seed(2019)
```

```{r, include = F}
library(tidyverse)
library(caret)

```

## data and manipulation
```{r}
cancer = read_csv("breast-cancer.csv") %>% 
  mutate(diagnosis = as.numeric(factor(diagnosis, levels = c("B","M"), labels = c(0,1)))-1) %>% 
  mutate(radius_mean = scale(radius_mean))
predictor_scale = as.tibble(scale(cancer[3:12]))
cancer_scale = cbind(rep(1,569),predictor_scale, cancer$diagnosis)
names(cancer_scale) = c("ones", names(cancer)[3:12], "response")

## standardize data!!
```

$$\log(\frac{\pi_i}{1-\pi_i}) = \mathbf X_i^T \boldsymbol \beta$$

```{r}
hessian_gradient_log = function(data, beta_vec){
  y = as.matrix(data%>% select(last_col()))
  x = as.matrix(data[,1:dim(data)[2]-1])
  theta = x%*%beta_vec
  pi = exp(theta)/(1+exp(theta))
  loglikelihood = sum(y*theta-log(1+exp(theta)))
  gradient = t(x)%*%(y-pi)
  pi_matrix = matrix(0, nrow = dim(data)[1],ncol = dim(data)[1])
  diag(pi_matrix) = pi*(1-pi)
  hessian = -t(x)%*%pi_matrix%*%(x)
  return(list(loglikelihood = loglikelihood, gradient = gradient, hessian = hessian))
  
}

a = hessian_gradient_log(cancer_scale, rep(0.02,11))

```

```{r}
NR = function(data, beta_start,tol = 1e-10, max = 200){
  i = 0
  cur = beta_start
  stuff = hessian_gradient_log(data, cur)
  curlog = stuff$loglikelihood
  res = c(i = 0,curlog = curlog,cur = cur)
  prevlog = -Inf
  while((i<=max)&(abs(curlog-prevlog)>tol)){
    i = i+1
    prevlog = stuff$loglikelihood
    eigen = eigen(stuff$hessian)
    if(sum(eigen$values)==0){
      hessian = stuff$hessian
    }
    else{
      hessian = stuff$hessian - max(eigen$values)
    }
    prev = cur
    cur = prev - solve(hessian)%*%stuff$gradient
    stuff = hessian_gradient_log(data,cur)
    curlog = stuff$loglikelihood
    res = rbind(res, c(i=i, curlog = curlog, cur = cur))
  }
  return(res)
}

NR(cancer_scale, rep(0.02,11))
```

# validation using glm
```{r}
cancer_fit = glm(cancer$diagnosis~cancer_scale, family = binomial(link = "logit"))
summary(cancer_fit)

# same results with NR
```

## questions or modify:

1. normalize or standardize?
2. how to standardize easily?

##
```{r}
lambda_max = max(abs(coef(cancer_fit))) ## warm start
tuning_grid = seq(10, 1, length = 100) ## tuning seq
## function of lasso coordinate descent algorithm
# input:
# data: y -- binary response;
#       x -- scaled predictors
# k: # fold for CV
# beta_vec: initial beta guess
# tune_grid: seq of lambda used in variable selection
# tol: tolerance for stop iteration
# max: max iteration times

# output: res--result list containing:
# beta -- tibble:
#     rows: lambdas
#     columns:
#     beta1-beta10: beta estimation for every lambda
#     targ: target function value for every lambda iteration
#     times: times of iteration in each lambda iteration
# coeff -- list of final coefficient
# best_tune -- best tuning parameter lambda
# MSE_te -- test MSE results among cv

lasso_co_des = function(data, beta_vec,  k = 5, tune_grid,tol = 1e-10, max = 200){
  ## create fold
  folds = createFolds(data$response,k = 5, returnTrain = T)
  
  ## lasso coord des for train data
  for (i in 1:k) {
    tr_rows = folds[[i]]
    train = data[tr_rows,]
    test = data[-tr_rows,]
    n_tr = dim(train)[1]
    x_tr = as.matrix(train[,1:dim(train)[2]-1]) # dim = (0.8*569, 11)
    y_tr = as.matrix(train[,dim(train)[2]]) # dim = (0.8*569, 1)
    ## for every lambda
    for (lambda in tune_grid) {
      theta_vec = x_tr%*%beta_vec # dim = (455,1), theta for every obs
      pi_vec = exp(theta_vec)/(1+exp(theta_vec)) # dim = (455,1), pi for every obs
      w_vec = pi_vec*(1-pi_vec) # dim = (455,1), weight for every obs 
      z_vec = theta_vec + (y_tr - pi_vec)/w_vec # dim = (455,1), working response for every obs 
      w_res_vec = sqrt(w_vec)*(z_vec - theta_vec)
      cur_target = (-1/2*n_tr)*t((w_res_vec))%*%(w_res_vec) + lambda*sum(abs(beta_vec[-1]))
      pre_target = -Inf
      time = 0
      res = c(k = k,lambda = lambda,time = time, cur_target = cur_target, beta_vec = beta_vec)
      while((abs(pre_target-cur_target)>tol) & time < max){
        time = time+1
        pre_target = cur_target
        beta_pre = beta_vec
         for (j in 1:dim(train)[2]-1) {
           w_z_vec_j = as.vector(sqrt(w_vec)*(z_vec - x_tr%*%beta_pre + as.vector(x_tr[,j]*beta_pre[j])))
           # dim(455,1),working response with out jth predictor
           w_x_tr_j = as.vector(as.vector(sqrt(w_vec)) * x_tr[,j])
           ## dim(455,1), weighted obs on jth row
           tmp = as.numeric(t(w_x_tr_j) %*% w_z_vec_j/(t(w_x_tr_j) %*% w_x_tr_j))
           if (j>=2) {
             if (abs(tmp)>lambda) {
             if(tmp > 0) {tmp = tmp - lambda}
             else {tmp = tmp + lambda}
           }else{tmp = 0}
           } 
           beta_pre[j] = tmp
           }
        ## sparse updating?
        beta_vec = beta_pre
        theta_vec = x_tr%*%beta_vec # dim = (455,1), theta for every obs
        pi_vec = exp(theta_vec)/(1+exp(theta_vec)) # dim = (455,1), pi for every obs
        w_vec = pi_vec*(1-pi_vec) # dim = (455,1), weight for every obs 
        z_vec = theta_vec + (y_tr - pi_vec)/w_vec 
        # dim = (455,1), working response for every obs 
        w_res_vec = sqrt(w_vec)*(z_vec - theta_vec)
        cur_target = (-1/2*n_tr)*t((w_res_vec))%*%(w_res_vec) +lambda*sum(abs(beta_vec[-1]))
        res = rbind(res,c(k = k,lambda = lambda,time = time, cur_target = cur_target, beta_vec))
        }
    }
  }
  res = as.tibble(res)
  return(res)
}
x = lasso_co_des(data = cancer_scale, beta_vec = rep(0.02,11), k = 1,tune_grid = tuning_grid,max = 500)
x

for (lambda in tuning_grid) {
  print(lambda)
}
```

## weight is 0 then the theta is 0 which is in denominator, how to fix?

## lambda??





