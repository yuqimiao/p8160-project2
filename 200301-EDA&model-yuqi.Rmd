---
title: "200301-EDA_and_model-yuqi"
author: "Yuqi Miao ym2771"
date: "3/1/2020"
output: html_document
---

---
title: "200301-EDA_and_model-yuqi"
author: "Yuqi Miao ym2771"
date: "3/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

require(survival)
require(quantreg)
require(glmnet)
require(MASS)
require(pROC)

set.seed(2019)
```

```{r, include = F}
library(tidyverse)
library(caret)

```

## data and manipulation
```{r}
cancer = read_csv("breast-cancer.csv") %>% 
  mutate(diagnosis = as.numeric(factor(diagnosis, levels = c("B","M"), labels = c(0,1)))-1) %>% 
  mutate(radius_mean = scale(radius_mean))
predictor_scale = as.tibble(scale(cancer[3:12]))
cancer_scale = cbind(rep(1,569),predictor_scale, cancer$diagnosis)
names(cancer_scale) = c("ones", names(cancer)[3:12], "response")

## standardize data!!
```

$$\log(\frac{\pi_i}{1-\pi_i}) = \mathbf x_i \boldsymbol \beta$$

```{r}
hessian_gradient_log = function(data, beta_vec){
  y = as.matrix(data%>% select(last_col()))
  x = as.matrix(data[,1:dim(data)[2]-1])
  theta = x%*%beta_vec
  pi = exp(theta)/(1+exp(theta))
  loglikelihood = sum(y*theta-log(1+exp(theta)))
  gradient = t(x)%*%(y-pi)
  pi_matrix = matrix(0, nrow = dim(data)[1],ncol = dim(data)[1])
  diag(pi_matrix) = pi*(1-pi)
  hessian = -t(x)%*%pi_matrix%*%(x)
  return(list(loglikelihood = loglikelihood, gradient = gradient, hessian = hessian))
  
}

a = hessian_gradient_log(cancer_scale, rep(0.02,11))

```

```{r}
NR = function(data, beta_start,tol = 1e-10, max = 200){
  i = 0
  cur = beta_start
  stuff = hessian_gradient_log(data, cur)
  curlog = stuff$loglikelihood
  res = c(i = 0,curlog = curlog,cur = cur)
  prevlog = -Inf
  while((i<=max)&(abs(curlog-prevlog)>tol)){
    i = i+1
    prevlog = stuff$loglikelihood
    eigen = eigen(stuff$hessian)
    if(sum(eigen$values)==0){
      hessian = stuff$hessian
    }
    else{
      hessian = stuff$hessian - max(eigen$values)
    }
    prev = cur
    cur = prev - solve(hessian)%*%stuff$gradient
    stuff = hessian_gradient_log(data,cur)
    curlog = stuff$loglikelihood
    res = rbind(res, c(i=i, curlog = curlog, cur = cur))
  }
  return(res)
}

NR_result = NR(cancer_scale, rep(0.02,11))

NR_coeff =  NR_result[dim(NR_result)[1], -1:-2]
```

# validation using glm
```{r}
cancer_fit = glm(response~., data = cancer_scale,family = binomial(link = "logit"))
summary(cancer_fit)

# same results with NR
```

## questions or modify:

1. normalize or standardize?
2. how to standardize easily?

##
```{r}
lambda_max = max(abs(NR_coeff)) ## warm start
tuning_grid = seq(lambda_max, 1, length = 100) ## tuning seq
## function of lasso coordinate descent algorithm
# input:
# data: y -- binary response;
#       x -- scaled predictors
# k: # fold for CV
# beta_vec: initial beta guess
# tune_grid: seq of lambda used in variable selection
# tol: tolerance for stop iteration
# max: max iteration times

# output: res--result list containing:
# beta -- tibble:
#     rows: lambdas
#     columns:
#     beta1-beta10: beta estimation for every lambda
#     targ: target function value for every lambda iteration
#     times: times of iteration in each lambda iteration
# coeff -- list of final coefficient
# best_tune -- best tuning parameter lambda
# MSE_te -- test MSE results among cv

lasso_co_des = function(data, beta_vec,  k = 5, tune_grid,tol = 1e-10, max = 200){
  ## create fold
  folds = createFolds(data$response,k = k, returnTrain = T)
  cv_result = c(k = 0,best_lambda = 0, beta_vec = beta_vec, g.stat_tr = Inf, g.stat_te = Inf)
  ## lasso coord des for train data
  for (i in 1:k) {
    tr_rows = folds[[i]]
    train = data[tr_rows,]
    test = data[-tr_rows,]
    n_tr = dim(train)[1]
    x_tr = as.matrix(train[,1:dim(train)[2]-1]) # dim = (0.8*569, 11)
    y_tr = as.matrix(train[,dim(train)[2]]) # dim = (0.8*569, 1)
    x_te = as.matrix(test[,1:dim(test)[2]-1]) # dim = (0.2*569, 11)
    y_te = as.matrix(test[,dim(test)[2]]) # dim = (0.2*569, 1)
    res = c()
    ## for every lambda
    for (lambda in sort(tune_grid, decreasing = T)) {
      theta_vec = x_tr%*%beta_vec # dim = (455,1), theta for every obs
      pi_vec = exp(theta_vec)/(1+exp(theta_vec)) # dim = (455,1), pi for every obs
      w_vec = pi_vec*(1-pi_vec) # dim = (455,1), weight for every obs 
      z_vec = theta_vec + (y_tr - pi_vec)/w_vec # dim = (455,1), working response for every obs 
      w_res_vec = sqrt(w_vec)*(z_vec - theta_vec)
      cur_target = (-1/2*n_tr)*t((w_res_vec))%*%(w_res_vec) + lambda*sum(abs(beta_vec[-1]))
      pre_target = -Inf
      time = 0
      names(beta_vec) = paste("beta_",0:10, sep = "")
      res = rbind(res,c(k = k,lambda = lambda,time = time, cur_target = cur_target, beta_vec))
      while((abs(pre_target-cur_target)>tol) & time < max){
        time = time+1
        pre_target = cur_target
        beta_pre = beta_vec
         for (j in 1:dim(train)[2]-1) {
           w_z_vec_j = as.vector(sqrt(w_vec)*(z_vec - x_tr%*%beta_pre + as.vector(x_tr[,j]*beta_pre[j])))
           # dim(455,1),working response with out jth predictor
           w_x_tr_j = as.vector(as.vector(sqrt(w_vec)) * x_tr[,j])
           ## dim(455,1), weighted obs on jth row
           tmp = as.numeric(t(w_x_tr_j) %*% w_z_vec_j/(t(w_x_tr_j) %*% w_x_tr_j))
           if (j>=2) {
             if (abs(tmp)>lambda) {
             if(tmp > 0) {tmp = tmp - lambda}
             else {tmp = tmp + lambda}
           }else{tmp = 0}
           } 
           beta_pre[j] = tmp
           }
        ## sparse updating?
        beta_vec = beta_pre
        theta_vec = x_tr%*%beta_vec # dim = (455,1), theta for every obs
        pi_vec = exp(theta_vec)/(1+exp(theta_vec)) # dim = (455,1), pi for every obs
        w_vec = pi_vec*(1-pi_vec) # dim = (455,1), weight for every obs 
        z_vec = theta_vec + (y_tr - pi_vec)/w_vec 
        # dim = (455,1), working response for every obs 
        w_res_vec = sqrt(w_vec)*(z_vec - theta_vec)
        cur_target = (-1/2*n_tr)*t((w_res_vec))%*%(w_res_vec) +lambda*sum(abs(beta_vec[-1]))
        res = rbind(res,c(k = k,lambda = lambda,time = time, cur_target = cur_target, beta_vec))
      }
    }
  ## choose lambda
  res = as.tibble(res)
  beta_lambda = res %>% 
    group_by(lambda) %>% 
    filter(cur_target == max(cur_target)) %>%  
    dplyr::select(contains("beta"))
  beta_lambda_m = as.matrix(beta_lambda[2:dim(beta_lambda)[2]]) # dim = (194, 11)
  ## use train dataset to choose lambda
  pi_hat = exp(x_tr %*% t(beta_lambda_m))/(1+exp(x_tr %*% t(beta_lambda_m)))
  #residual_lambda = rep(y_tr,dim(beta_lambda_m)[1]) - pi_hat
  #MSE_train = as.vector(rowSums(t(residual_lambda^2)))
  g.res = (rep(y_tr,dim(beta_lambda_m)[1]) - pi_hat)/sqrt(pi_hat*(1-pi_hat))
  g.stat_tr = as.vector(rowSums(t(g.res^2)))
  best_result = as.tibble(cbind(beta_lambda,g.stat_tr = g.stat_tr)) %>% 
    filter(g.stat_tr == min(g.stat_tr)) 
  
  # use test datasets to evaluate
  best_result_m = as.matrix(best_result[3:dim(best_result)[2]-1])
  pi_hat_te = exp(x_te %*% t(best_result_m))/(1+exp(x_te %*% t(best_result_m)))
  #residual_test = y_te - pi_hat_test
  #MSE_test = as.vector(rowSums(t(residual_test^2)))
  g.res_te = (y_te - pi_hat_te)/sqrt(pi_hat_te*(1-pi_hat_te))
  g.stat_te = as.vector(rowSums(t(g.res_te^2)))
  
  cv_result = rbind(cv_result, c(k = i,best_lambda = best_result$lambda, beta_vec = best_result[3:dim(best_result)[2]-1], g.stat_tr = best_result$g.stat_tr, g.stat_te = g.stat_te))
  #cv_result = as.tibble(cv_result)  
  }
  
  return(cv_result)
}
x = lasso_co_des(data = cancer_scale, beta_vec = rep(0.02,11), k = 5,tune_grid = tuning_grid,max = 200)

x

```

## instead of using MSE, using pearson chi-square 













