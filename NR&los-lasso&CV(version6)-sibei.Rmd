---
title: "group_project2"
author: "Sibei Liu sl4660"
date: "2020/3/1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(ggrepel)
require(survival)
require(quantreg)
require(glmnet)
require(MASS)
require(pROC)

set.seed(2019)
data_cancer=read_csv("breast-cancer.csv") %>% 
  mutate(
    diagnosis=as.numeric(factor(diagnosis,level=c("B","M")))-1
  ) 

n=nrow(data_cancer)
predictor_scaled=scale(data_cancer[,3:12])
x_matrix_nointer=as.matrix(predictor_scaled)
x_matrix=as.matrix(cbind(rep(1,n),predictor_scaled))# add 1 as to combine with beta_0
y_matrix=data_cancer$diagnosis

```


```{r}
### create function to obtain loglike, gradient and hess

func_log= function(x_matrix,beta_estimate){
theta=x_matrix%*%t(t(beta_estimate))
pi=exp(theta)/(1+exp(theta))
## below loglike
loglike=t(y_matrix)%*%theta-sum(log(1+exp(theta)))
## below gradient
gradient=t(x_matrix)%*%(t(t(y_matrix))-pi)

## below hess
c=pi*(1-pi)
dia_pi_matrix=matrix(rep(0,n*n),n,n)
  i=1
  for(i in 1:n){
    dia_pi_matrix[i,i]=c[i]
    i=i+1}
  
 hess=-t(x_matrix)%*%dia_pi_matrix%*%x_matrix
return(list(loglike=loglike,gradient=gradient,hess=hess))
}


d=func_log(x_matrix,rep(0.02,11))
d
```


modify the direction and step using step halving if necessary
```{r}

newton_direc =function(data,func,start,tol=exp(-10),max=200){
  i=0
  cur=start
  stuff=func(data,cur)
  prevloglike=-Inf
  res=c(0,stuff$loglike,cur,1)
  
  while((i<max) && (abs(stuff$loglike-prevloglike)>tol) ){
    eigen_value=eigen(stuff$hess)
    
    if(eigen_value$values[1]>0 |eigen_value$values[2]>0|eigen_value$values[3]>0|eigen_value$values[4]>0
       |eigen_value$values[5]>0|eigen_value$values[6]>0|eigen_value$values[7]>0|eigen_value$values[8]>0|eigen_value$values[9]>0|eigen_value$values[10]>0|eigen_value$values[11]>0)
      {
      stuff$hess=stuff$hess-max(eigen_value$values)*matrix(rep(1,11*11),11,11)
    }### to check the if hess is negative definite, correct the direction
    
    
    i=i+1
    prevloglike=stuff$loglike
    pre=cur
    cur=pre-solve(stuff$hess) %*% stuff$gradient
    stuff=func(data,cur)
    n=0
    #while((stuff$loglike <= prevloglike)){
    while((stuff$loglike <= prevloglike) && (n<10)){
      n=n+1
      cur=pre-(1/(2^n))*(solve(stuff$hess))%*% stuff$gradient
      stuff=func(data,cur)
    }## above loop is  step halving
    res=rbind(res,c(i,stuff$loglike,cur,1/2^n))
  }
  return(res)
}


# starting value rep(0.02,11)
result3=newton_direc(x_matrix,func_log,rep(0.02,11))
result3
result3[nrow(result3),]
```




# Logistic Lasso
```{r}
ori_linear=glm(y_matrix~x_matrix[,-1],family=binomial(link='logit'))
coef_1=coef(ori_linear) %>% as.vector()
lambda_max=max(abs(coef(ori_linear)))
lambdas=seq(14, 0.01,length = 100)
len=length(lambdas)

# s function
    S=function(solution,c){
  if (abs(solution)>c){
    if(solution<0) {return(solution=solution+c)}
    else{return(solution=solution-c)}
  }
  else{return(0)}
    }

    
   #S_S=function(solution,lambda) {
    # i=1
     #for(i in 1:length(solution)){
     #if (abs(solution[i])>lambda){
    #if(solution[i]<0){
     # solution[i]=solution[i]+lambda
    #}else{solution[i]=solution[i]-lambda}
  #}
  #else{solution[i]=0}
   #    i=i+1
     #}
     #return(solution)
   #}

   
```


## solution of betas of each lambda
```{r}

# working stuff
 quadratic_coor_wise_once = function(beta_vec,x_matrix,y_matrix,lambda){
  x = x_matrix
  y = y_matrix
  pi=exp(x%*%(beta_vec))/(1+exp(x%*%beta_vec))
  wi=pi*(1-pi)
  zi=x%*%beta_vec+(y-pi)/wi
  
  
  y_star1=zi-x[,-1]%*%beta_vec[-1]
  beta=rep(0,11)# to store
  beta[1]=sum(wi*y_star1)/sum(wi)
  beta_update=c(beta[1],beta_vec[2:11])
  i=2
  for(i in 2:11){
    y_star2=zi-x[,-i]%*% beta_update[-i]
    beta[i]=sum(wi*x[,i]*y_star2)
    beta[i]=S(beta[i],(lambda*length(y)))/sum(wi*(x[,i]^2))
    beta_pre=beta_update
    if(i!=11){beta_update=c(beta_pre[1:i-1],beta[i],beta_pre[(i+1):11])}else
    {beta_update = c(beta_pre[1:i-1],beta[i])}
    i=i+1
  }
 
    cur_tar=1/(2*length(y))*(sum(wi*(zi-x%*%beta_update)^2))+lambda*sum(abs(beta_update[-1]))
  
 return(c(beta_update,cur_tar))
}

 beta_2=rep(0.02,11)
 quadratic_coor_wise_once(beta_2,x_matrix,y_matrix,0.01)
```


```{r}
## only return the betas that reach the minmun target value for this lambda
 quadratic_coor_wise_all=function(beta_enter,x_matrix,y_matrix,lambda,tol=exp(-10),max=200){
  x = x_matrix
  y = y_matrix
  pi=exp(x%*%(beta_enter))/(1+exp(x%*%beta_enter))
  wi=pi*(1-pi)
  zi=x%*%beta_enter+(y-pi)/wi
  
  cur=1/(2*length(y))*(sum(wi*(zi-x%*%beta_enter)^2))+lambda*sum(abs(beta_enter[-1]))
  pre=Inf
  beta_3=beta_enter
  iteration=0
  result4=c(beta_enter,cur)
  names(result4)=c(str_c("beta",0:10),"cur_tar")
 while((abs(cur-pre)>tol)&(cur<pre) & iteration < max){
   pre=cur
  iteration=iteration+1
   result_new=quadratic_coor_wise_once(beta_3,x,y,lambda)
   beta_new=result_new[1:11]
   cur=result_new[12]
  result4=rbind(result4,result_new)
  beta_3=beta_new
 }
  c1=dim(result4)[1]
  return(result4[c1-1,1:11]) }

beta_enter=rep(0.02,11)
quadratic_coor_wise_all(beta_enter,x_matrix,y_matrix,0)

```

cross-validation
```{r}

data_use=data_cancer 

set.seed(2019)
library(caret)
library(ModelMetrics)
#Split data to 5 fold
cvSplit = createFolds(data_use$diagnosis,
                             k=5,
                             returnTrain = TRUE)
str(cvSplit)
fold_number=5
```




```{r}
lambdas2=seq(3, 0,length = 100)
len2=length(lambdas2)
```

create empty mse_folder to hold the mse in each lambda and each fold
```{r}
mse_folds=matrix(rep(NA,fold_number*len2),nrow=fold_number,ncol=len2)
auc_folds=matrix(rep(NA,fold_number*len2),nrow=fold_number,ncol=len2)
```


```{r}
k=1
library(pROC)
## initial guess = beta_2=rep(0.02,11)
for(k in 1:fold_number){
  
  trRows=cvSplit[[k]]
  train_fold=data_use[trRows,]
  vali_fold= data_use[-trRows,]
  
  x_train = as.matrix(cbind(rep(1,nrow(train_fold)),scale(train_fold[,3:12])))
  y_train = train_fold$diagnosis

  x_vali = as.matrix(cbind(rep(1,nrow(vali_fold)),scale(vali_fold[,3:12]))) 
  y_vali = vali_fold$diagnosis
 
  d=1
  for(d in 1:(len2)){
  result=quadratic_coor_wise_all(beta_enter,x_train,y_train,lambdas[d])
  theta= x_vali%*% t(t(result))
  y_fit=exp(theta)/(1+exp(theta))
  mse_folds[k,d]=mse(y_vali,y_fit)
  d=d+1
}
  k=k+1
}

mse_folds
```


```{r}
mse_final=c(rep(NA,len2))
i=1
for(i in 1:len2){
  mse_final[i]=mean(mse_folds[,i])
  i+1}

mse_final



mins=min(mse_final)
mins
order=which(mse_final==mins)
lambdas2[order]## is the lambda which produces the smallest mse

dataframe=tibble(mse=mse_final,
                        lambdas=lambdas2)
ggplot(dataframe,aes(x=lambdas,y=mse))+geom_line()

dataframe1=tibble(mse=mse_final,
                        lambdas=lambdas2) %>% filter(lambdas2<=3)

  ggplot(dataframe1,aes(x=lambdas,y=mse,label=mse))+geom_point(color = ifelse(dataframe1$lambdas ==lambdas2[order],"red", "grey" ))
  
```


```{r}
set.seed(2019)
cancer_package=data_cancer[,2:12]
x.mat <- model.matrix(diagnosis~., cancer_package)[,-1]
y.class <- cancer_package$diagnosis

ctrl1 <- trainControl(method = "cv", number = 5)
lasso.fit <- train(x.mat, y.class,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = seq(3,-1,length=100)), 
                   # preProc = c("center", "scale"),
                   trControl = ctrl1)

lasso.fit$bestTune
plot(lasso.fit)
min(lasso.fit$results$RMSE)
co=coef(lasso.fit$finalModel,lasso.fit$bestTune$lambda)
```


