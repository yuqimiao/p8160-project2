---
title: "group_project2"
author: "Sibei Liu sl4660"
date: "2020/3/1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)

require(survival)
require(quantreg)
require(glmnet)
require(MASS)
require(pROC)

set.seed(2019)
data_cancer=read_csv("breast-cancer.csv") %>% 
  mutate(
    diagnosis=as.numeric(factor(diagnosis,level=c("B","M")))-1
  )
n=nrow(data_cancer)
predictor_scaled=scale(data_cancer[,3:12])
x_matrix=as.matrix(cbind(rep(1,n),predictor_scaled))# add 1 as to combine with beta_0
y_matrix=data_cancer$diagnosis

```


```{r}
### create function to obtain loglike, gradient and hess

func_log= function(x_matrix,beta_estimate){
theta=x_matrix%*%t(t(beta_estimate))
pi=exp(theta)/(1+exp(theta))
## below loglike
loglike=t(y_matrix)%*%theta-sum(n*log(1+exp(theta)))
## below gradient
gradient=t(x_matrix)%*%(t(t(y_matrix))-pi)

## below hess
c=pi*(1-pi)
dia_pi_matrix=matrix(rep(0,n*n),n,n)
  i=1
  for(i in 1:n){
    dia_pi_matrix[i,i]=c[i]
    i=i+1}
  
 hess=-t(x_matrix)%*%dia_pi_matrix%*%x_matrix
return(list(loglike=loglike,gradient=gradient,hess=hess))
}


d=func_log(x_matrix,rep(0.4,11))
d$hess
eigen(d$hess)
```


modify the direction and step using step halving if necessary
```{r}
newton_direc =function(data,func,start,tol=exp(-10),max=200){
  i=0
  cur=start
  stuff=func(data,cur)
  prevloglike=-Inf
  res=c(0,stuff$loglike,cur,1)
  
  while((i<max) && (abs(stuff$loglike-prevloglike)>tol) ){
    eigen_value=eigen(stuff$hess)
    
    if(eigen_value$values[1]>0 |eigen_value$values[2]>0|eigen_value$values[3]>0|eigen_value$values[4]>0
       |eigen_value$values[5]>0|eigen_value$values[6]>0|eigen_value$values[7]>0|eigen_value$values[8]>0|eigen_value$values[9]>0|eigen_value$values[10]>0|eigen_value$values[11]>0)
      {
      stuff$hess=hess-max(eigen_value$values)*matrix(rep(1,11*11),11,11)
    }### to check the if hess is negative definite, correct the direction
    
    i=i+1
    prevloglike=stuff$loglike
    pre=cur
    cur=pre-solve(stuff$hess) %*% stuff$gradient
    stuff=func(data,cur)
    n=0
    while((stuff$loglike <= prevloglike) && (n<10)){
      n=n+1
      cur=pre-(1/(2^n))*(solve(stuff$hess))%*% stuff$gradient
      stuff=func(data,cur)
    }## above loop is  step halving
    res=rbind(res,c(i,stuff$loglike,cur,1/2^n))
  }
  return(res)
}

# starting value rep(0.1,11)
result2=newton_direc(x_matrix,func_log,rep(0.4,11))
result2
result2[nrow(result2),]
# starting value rep(0.2,11)
result3=newton_direc(x_matrix,func_log,rep(0.2,11))
result3
result3[nrow(result3),]

```

# Logistic Lasso
```{r}
ori_linear=glm(y_matrix~x_matrix[,-1],family=binomial(link='logit'))
lambda_max=max(coef(ori_linear))
lambdas=seq(lambda_max,0,by=-2)
len=length(lambdas)

# s function
  S=function(solution,lambda){
  if (abs(solution)>lambda){
    if(solution<0){
      return(solution+lambda)
    }else{return(solution-lambda)}
  }
  else{return(0)}
}
```

## solution of betas of each lambda
```{r}
# when lambda=max_lambda
beta_1=rep(0,11)


# working stuff
 quadratic_appro = function(beta_vec,x_matrix,y_matrix,lambda){
  x = x_matrix
  y = y_matrix
  pi=exp(x%*%(beta_vec))/(1+exp(x%*%beta_vec))
  wi=pi*(1-pi)
  zi=x%*%beta_vec+(y-pi)/wi
  
  y_star1=zi-x%*%beta_vec
  beta=rep(0,11)
  beta[1]=mean(y_star1)
  beta_update=c(beta[1],beta_vec[2:11])
  i=2
  for(i in 2:11){
    y_star2=zi-x[,-i]%*% beta_update[-i]
    beta[i]=sum(wi*x[,i]*y_star2)
    beta[i]=S(beta[i],lambdas[2])/sum(wi*(x[,i]^2))
    beta_pre=beta_update
    if(i!=11){beta_update=c(beta_pre[1:i-1],beta[i],beta_pre[(i+1):11])}else
    {beta_update = c(beta_pre[1:i-1],beta[i])}
    i=i+1
  }
 return(beta_update)
}

 quadratic_appro(beta_1,x_matrix,y_matrix,lambdas[2])
```

# use all x and y to try the  quadratic_appro and get the series of betas
```{r}
lab=c("lambda",str_c("beta",1:11))
beta_new=beta_1
result4=rbind(lab,c(lambdas[1],beta_new))
i=2
for(i in 2:len){
  beta_new=quadratic_appro(beta_new,x_matrix,y_matirx,lambdas[i])
  result4=rbind(result4,c(lambdas[i],beta_new))
  i=i+1
}
result4 %>% tibble()
```

Cross-validation
```{r}
data_use=data_cancer 

set.seed(2020)
library(caret)
library(ModelMetrics)
#Split data to 5 fold
cvSplit = createFolds(data_use$diagnosis,
                             k=5,
                             returnTrain = TRUE)
str(cvSplit)
fold_number=5
```


create empty mse_folder to hold the mse in each lambda and each fold
```{r}
mse_folds=matrix(rep(NA,fold_number*len),nrow=fold_number,ncol=len)
```


```{r}
k=1
for(k in 1:fold_number){
  trRows=cvSplit[[k]]
  train_fold=data_use[trRows,]
  vali_fold= data_use[-trRows,]
  
  x_train = as.matrix(cbind(rep(1,nrow(train_fold)),scale(train_fold[,3:12])))
  y_train = train_fold$diagnosis

  x_vali = as.matrix(cbind(rep(1,nrow(vali_fold)),scale(vali_fold[,3:12]))) 
  y_vali = vali_fold$diagnosis
  
  
  lab=c("lambda",str_c("beta",1:11))
 beta_new=rep(0,11)
 result5=rbind(lab,c(lambdas[1],beta_new))
 i=2
 for(i in 2:len){
  beta_new=quadratic_appro(beta_new,x_train,y_train,lambdas[i])
  result5=rbind(result5,c(lambdas[i],beta_new))
  i=i+1
}
  d=2
  for(d in 2:(len+1)){
  beta_cor=as.numeric(result5[d,2:12])
  theta= x_vali%*% t(t(beta_cor))
  y_fit=exp(theta)/(1+exp(theta))
  mse_folds[k,d-1]=mse(y_vali,y_fit)
  d=d+1
}
  k=k+1
}

mse_folds

mse_final=c(rep(NA,len))
for(i in 1:len){
  mse_final[i]=mean(mse_folds[,i])}

mse_final
mins=min(mse_final)

order=which(mse_final==mins)
lambdas[order]## is the lambda which produces the smallest mse
```



