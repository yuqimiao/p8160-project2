---
title: "Project2 :Breast Cancer Prediction Model "
author: "Group 7: Melanie Mayer, Yuqi Miao, Sibei Liu, Xue Jin"
date: "March 31, 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Breast cancer is the most common invasive cancer and the second leading cause of death from cancer in women worldwide, marked by the uncontrolled growth of breast cells. Non-cancerous breast tumors do not metastasize and are usually not life-threatening, while malignant tumors are cancerous, aggressive and deadly. Therefore, it’s important to have breast lumps accurately diagnosed so that decision with regard to medical treatment, rehabilitation and personal matters can be made appropriately. 

## Objectives

The main objective of this project is to build an accurate predictive model based on logistic regression that classifies between malignant and benign images of breast tissue. Using the Breast Cancer Diagnosis dataset, logistics model and logistic-LASSO model will be implemented to predict the diagnosis. A Newton-Raphson algorithm and Pathwise Coordinate optimization will be developed to estimate the logistic model and the lasso model respectively. We aim to find the model with the best performance in terms of predicting when breast tissue is malignant. Ten major predictors will be included in models presented in the result section and models with all 30 predictors are reported in supplementary section. 

## Breast Cancer Diagnosis Dataset

The Breast Cancer Diagnosis dataset contains the diagnosis and a set of 10 features capturing the characteristics of the cell nuclei present in the digitized image of breast mass. The ten features collected for each cell nucleus are:

* radius (mean of distances from center to points on the perimeter) 
* texture (standard deviation of gray-scale values) 
* perimeter 
* area 
* smoothness (local variation in radius lengths) 
* compactness (perimeter^2 / area - 1.0) 
* concavity (severity of concave portions of the contour) 
* concave points (number of concave portions of the contour) 
* symmetry 
* fractal dimension (“coastline approximation” - 1) 

The mean, standard deviation (SD) and largest values of these features are computed for each image, resulting in 30 possible predictive variables. We will analyze the predictive ability for diagnosis of malignant or benign cases of these covariates.

## Methods

### Model Parameters

For logistic regression we assume the response variable $Y_i$ for the $i_{th}$ observation follows a binary distribution:

$$Y_i\sim Bin(\pi_i)$$
where $\pi_i$ denotes the probablity that the $i_{th}$ observation's tissue is malignant. We can assume all observations are independent from one another, hence the likelihood function for the vector $\boldsymbol\pi$ can be written as:

$$L(\boldsymbol\pi)=\prod_{i=1}^n f(y_i)=\prod_{i=1}^n\pi_i^{y_i}(1 - \pi_i)^{1-y_i}$$

For logistic regression, the logit link function is used: $$\log(\frac{\pi_i}{1-\pi_i}) = \boldsymbol \beta ^T\boldsymbol x_i = \theta_i$$ where $\boldsymbol x_i^T =  \begin{bmatrix} 1 & x_{1i} &x_{2i} & ... & x_{pi} \end{bmatrix}$ and $\boldsymbol \beta^T =  \begin{bmatrix} \beta_0 & \beta_1  & \beta_2 & ... & \beta_p \end{bmatrix}$. One can solve for $\pi_i = \frac{e^{\theta_i}}{1 + e^{\theta_i}}$. We aim to find the best estimate of the vector of coefficients, $\boldsymbol \beta$. The log-likelihood function for this vector can be written as:

$$l(\boldsymbol\theta)=logL(\boldsymbol\pi) =\sum_{i=1}^n(Y_ilog\frac{\pi_i}{1-\pi_i}+log(1-\pi_i)) = \sum_{i=1}^n(Y_i\theta_i - log(1 + e^{\mathbf \theta_i}))\\$$
The maximum likelihood is thus achieved when the gradient is equal to zero and the Hessian is negative definite. The gradient can be found to be:

$$\nabla l(\boldsymbol\theta|\boldsymbol X) = \sum_{i=1}^n (Y_i - \pi_i)\boldsymbol x_i = \boldsymbol X^T(\boldsymbol Y - \boldsymbol \pi) $$

The Hessian matrix is thus:

$$\nabla^2 l(\boldsymbol\theta|\boldsymbol X) = -\sum_{i=1}^n \pi_i(1 - \pi_i)\boldsymbol x_i\boldsymbol x_i^T = -\boldsymbol X^Tdiag(\pi_i(1 - \pi_i)) \boldsymbol X$$


### Full model

In order to estimate $\boldsymbol \beta$ we need to maximize the loglikelihood function. There is no closed form, hence we turn to numerical methods. The Newton-Raphson method is used to fit the full model. To find the maximum likelihood estimate of each element of $\boldsymbol \beta$, an iterative process is set as follows:

$$\theta_{i+1}  = \theta_{i} -\delta (\nabla^2l(\theta_{i}|\boldsymbol X)-\gamma I)^{-1}\nabla l(\theta_{i}|\boldsymbol X) $$
This is the Newton-Raphson method with two modifications. $\delta$ is included in the process to accomplish the step-halving modification, the step coefficient ensures the likelihood function is always increasing in order to achieve quicker convergence. $\gamma$ is the modification coefficient to ensure the aescent direction of the iteration vector at $\theta_{i}$.


### Logit-lasso pathwise coordinate-wise update algorithm

In the case of large dimensionality of predictors or multicollinearity it can be beneficial to perform a regularization method which either shrinks coefficients or performs variable selection. Here we implement the Least Absolute Shrinkage and Selection Operator (LASSO) method for logistic regression with a path-wise coordinate-wise optimization algorithm to select variables from the full model and increase the prediction efficiency.

In LASSO we add a penalization term such that we try to find the coefficients to minimize:

$$min _{(\boldsymbol \beta)}\{-l({\boldsymbol \beta})+\lambda\sum_{j=0}^{p}|\beta_j| \}$$
Where we find the likelihood to be: 
$$l({\boldsymbol \beta}) = -\frac{1}{2n}\sum_{i=1}^{n}\omega_i(z_i-{\boldsymbol {X_i\beta}})^2$$

$$\pi_i = \frac{exp({\boldsymbol {X_i\beta}})}{1+exp({\boldsymbol {X_i\beta}})}$$

$$\omega_i = \pi_i(1-\pi_i)$$

$$z_i = {\boldsymbol {X_i\beta}}+\frac{y_i-\pi_i}{\pi_i(1-\pi_i)}$$

Pre-define the tuning parameter sequence $\{\lambda_1,...,\lambda_s\}$, starting point ${\boldsymbol{\beta_{start=}}}\{\beta_0^{(0)},...,\beta_p^{(0)}\}$. Here we elaborately explain the optimal process for $\lambda_u$: Using the optimal $\boldsymbol {\beta_{u-1}}$ from last iteration as the warm start. within every iteration, find the optimal $\beta$ coordinate-wise.
For $\beta_j$ in $t_th$ iteration

$$\beta_j^{(t)} = \left\{\begin{array}{lc} \frac{\sum_{i=1}^{n} \omega_i(z_i-\sum_{j=1}^{p}{\boldsymbol {X_i}\beta_j})}{\sum_{i}^{n}\omega_i},&j=0\\\frac{s(\beta_j^{(t*)},\lambda_un)}{\sum_{i=1}^{n}\omega_ix_{ij}^2},&j = 1,2,...,p\end{array}\right.$$

$$\beta_j^{(t*)} = \sum_{i=1}^{n}\omega_ix_{ij}z_{ij}^*$$
$$z_{ij}^* = z_i-\underset {\beta_k\neq0}{\sum_{k = 0}^{j-1}}\beta_k^{(i)}x_{ik}-\underset {\beta_k\neq0}{\sum_{k=j+1}^{p}}\beta_k^{(i-1)}x_{ik}$$

#### Cross Validation

In order to check the model performance, we use 5-fold cross validation. Using training dataset  to choose the best tuning parameter and fit model, and using test dataset to evaluate the final prediction. The statistics we use to compare the validation is SSE and pearson chi-square statistics, which are defined as:

$$SSE = \sum_{i=1}^{n}(y_i-{\widehat \pi_i})^2$$
$${\widehat \pi_i} = log\frac{exp({\boldsymbol X_i\beta})}{1+exp({\boldsymbol X_i\beta})}$$
$$G = \sum_{i=1}^{n}\frac{y_i-{\widehat \pi_i}}{{\widehat \pi_i}(1-{\widehat \pi_i})}$$

By taking average of above 2 statistics of 5 fold, we get the final index to evaluate the model fitting.

## Results
### Newton-Raphson

After doing the Newton-Raphson modified with step and direction, the estimation of coefficients are:

\begin{center}
\includegraphics{./results/NR.JPG}
\end{center}

\begin{center}
Table 1. Estimated coefficients under Newton-Raphson method
\end{center}

### Coordinate-Wise


The range of $\lambda$ we tried is (3,0) with length 100. The initial guess of all $\beta$ including the intercept is 0.02. The g.statistics, SSE, AUC are introduced to select best $\lambda$ in Cross Validation. The optimal $\lambda$ would be selected based on the minimum SSE, maxmimum AUC and minimum g-statistics in test data. Below is the results in 5-Fold Cross Validation:

\begin{center}
\includegraphics{./results/Results from Yuqi.JPG}
\end{center}

\begin{center}
Table 2. Cross validatin results
\end{center}

In all 5 folds, all three critria indicates the same optimal $\lambda$, whcih is 0.In the test data of each fold, the AUC ranges from 0.99 to 0.97, g-statistics ranges from  22.6 to 48.0, while SSE changes from 4.1 to 6.8.

\begin{center}
\includegraphics{./results/lambda_change.jpeg}
\end{center}

\begin{center}
Fig 1. Changes in three criteria vs $\lambda$
\end{center}

The Figure 1 shows the trend of AUC SSE and g-statistics in train data. With the increse of $\lambda$, both SSE and g-statistics have a dramatic soar. While AUC has a great drop from 1 to 0.5. All of them indicate that the bigger the $\lambda$ is, the worse the model would perform.

## Discussion
 
